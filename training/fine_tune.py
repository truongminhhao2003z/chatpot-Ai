# 1. IMPORTS
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, get_linear_schedule_with_warmup
import torch
from torch.utils.data import Dataset, DataLoader, random_split
from tqdm import tqdm
import json, re, os, logging
from datetime import datetime

# 2. LOGGING CONFIGURATION
log_dir = "training/logs"
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'{log_dir}/training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# 3. PREPROCESSING FUNCTIONS
def preprocess_input(text):
    """
    X·ª≠ l√Ω text ƒë·∫ßu v√†o:
    - chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng
    - lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng mong mu·ªën (ngo·∫°i tr·ª´ ti·∫øng Vi·ªát)
    """
    text = text.strip().lower()
    # Gi·ªØ l·∫°i ch·ªØ c√°i, s·ªë, d·∫•u c√°ch v√† c√°c k√Ω t·ª± ti·∫øng Vi·ªát c∆° b·∫£n
    text = re.sub(r'[^\w\s√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë]', '', text)
    return text

# 4. DATASET CLASS
class ChatbotDataset(Dataset):
    """Dataset t√πy ch·ªânh cho chatbot"""

    def __init__(self, data, tokenizer, max_length=128):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        input_text = f"chat: {item['input']}"
        target_text = item['output']

        # Tokenize input
        input_encodings = self.tokenizer(
            input_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        # Tokenize target (label)
        target_encodings = self.tokenizer(
            target_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        input_ids = input_encodings['input_ids'].squeeze()
        attention_mask = input_encodings['attention_mask'].squeeze()
        labels = target_encodings['input_ids'].squeeze()

        # B·ªè qua padding token trong loss calculation b·∫±ng c√°ch thay th·∫ø b·∫±ng -100
        labels[labels == self.tokenizer.pad_token_id] = -100

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }

# 5. MODEL EVALUATION
def evaluate_model(model, dataloader, device):
    """ƒê√°nh gi√° model tr√™n t·∫≠p validation"""
    model.eval()
    total_val_loss = 0
    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            total_val_loss += outputs.loss.item()

    return total_val_loss / len(dataloader)

# 6. MAIN TRAINING FUNCTION
def main():
    # 6.1 Setup thi·∫øt b·ªã v√† si√™u tham s·ªë
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\nüñ•Ô∏è ƒêang s·ª≠ d·ª•ng: {device}")
    logging.info(f"ƒêang s·ª≠ d·ª•ng thi·∫øt b·ªã: {device}")

    BATCH_SIZE = 2
    MAX_LENGTH = 64
    NUM_EPOCHS = 3

    cache_dir = "./model_cache"
    results_dir = "training/results"
    os.makedirs(results_dir, exist_ok=True)

    model_name_or_path = "vietai/vit5-base"

    # 6.2 Load model v√† tokenizer
    print(f"\n1Ô∏è‚É£ ƒêang t·∫£i model '{model_name_or_path}'...")
    logging.info(f"ƒêang t·∫£i model '{model_name_or_path}' t·ª´ Hugging Face Hub ho·∫∑c cache t·∫°i '{cache_dir}'...")

    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path, cache_dir=cache_dir).to(device)
        print("‚úÖ T·∫£i model th√†nh c√¥ng!")
        logging.info("T·∫£i model th√†nh c√¥ng.")
    except Exception as e:
        print(f"‚ùå L·ªói: {str(e)}")
        logging.error(f"L·ªói khi t·∫£i model: {str(e)}")
        print("Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi internet ho·∫∑c ƒë·∫£m b·∫£o m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i v·ªÅ ƒë·∫ßy ƒë·ªß trong cache.")
        return

    # 6.3 Load d·ªØ li·ªáu
    print("\n2Ô∏è‚É£ ƒêang t·∫£i d·ªØ li·ªáu...")
    logging.info("ƒêang t·∫£i d·ªØ li·ªáu t·ª´ 'training/data.json'...")

    try:
        with open('training/data.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"‚úÖ ƒê√£ t·∫£i {len(data)} m·∫´u d·ªØ li·ªáu")
        logging.info(f"ƒê√£ t·∫£i {len(data)} m·∫´u d·ªØ li·ªáu.")
    except FileNotFoundError:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file data.json! Vui l√≤ng ƒë·∫£m b·∫£o file t·ªìn t·∫°i.")
        logging.error("Kh√¥ng t√¨m th·∫•y file data.json!")
        return

    # 6.4 Chu·∫©n b·ªã dataset v√† dataloader
    print("\n3Ô∏è‚É£ Chu·∫©n b·ªã d·ªØ li·ªáu...")
    logging.info("ƒêang chu·∫©n b·ªã d·ªØ li·ªáu cho training v√† validation.")

    dataset = ChatbotDataset(data, tokenizer, max_length=MAX_LENGTH)

    train_size = int(0.9 * len(dataset))
    val_size = len(dataset) - train_size

    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

    train_dataloader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=2,
        pin_memory=True
    )
    val_dataloader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        pin_memory=True
    )
    logging.info(f"Ph√¢n chia d·ªØ li·ªáu: Train {len(train_dataset)} m·∫´u, Validation {len(val_dataset)} m·∫´u.")

    # 6.5 Optimizer v√† Scheduler
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=50,
        num_training_steps=len(train_dataloader) * NUM_EPOCHS
    )
    logging.info("ƒê√£ thi·∫øt l·∫≠p Optimizer v√† Scheduler.")

    # 6.6 V√≤ng l·∫∑p hu·∫•n luy·ªán
    print(f"\n4Ô∏è‚É£ B·∫Øt ƒë·∫ßu training...")
    print(f"üìä Train: {len(train_dataset)} m·∫´u, Validation: {len(val_dataset)} m·∫´u")
    print(f"üìà Batch size: {BATCH_SIZE}, Epochs: {NUM_EPOCHS}")
    logging.info(f"B·∫Øt ƒë·∫ßu training v·ªõi {NUM_EPOCHS} epochs, batch size {BATCH_SIZE}.")

    best_val_loss = float('inf')
    for epoch in range(NUM_EPOCHS):
        model.train()
        total_loss = 0
        progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}')

        for batch in progress_bar:
            outputs = model(
                input_ids=batch['input_ids'].to(device),
                attention_mask=batch['attention_mask'].to(device),
                labels=batch['labels'].to(device)
            )

            loss = outputs.loss
            total_loss += loss.item()
            loss.backward()

            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()

            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

        avg_train_loss = total_loss / len(train_dataloader)
        logging.info(f"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}")

        val_loss = evaluate_model(model, val_dataloader, device)
        print(f"\nüìä Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {val_loss:.4f}")
        logging.info(f"Epoch {epoch+1} - Val Loss: {val_loss:.4f}")

        # L∆∞u model t·ªët nh·∫•t
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            print("üíæ L∆∞u model t·ªët nh·∫•t...")
            logging.info(f"Val loss c·∫£i thi·ªán ({best_val_loss:.4f}), l∆∞u model t·ªët nh·∫•t.")
            model.save_pretrained(f"{results_dir}/best_model")
            tokenizer.save_pretrained(f"{results_dir}/best_model")
        else:
            logging.info(f"Val loss kh√¥ng c·∫£i thi·ªán. Best Val Loss hi·ªán t·∫°i: {best_val_loss:.4f}.")

    # 6.7 Test model
    print("\n5Ô∏è‚É£ Test model...")
    logging.info("B·∫Øt ƒë·∫ßu test model v·ªõi c√°c v√≠ d·ª•.")

    test_inputs = [
        "Xin ch√†o!",
        "bye bye!",
        "H√¥m nay th·ªùi ti·∫øt th·∫ø n√†o?",
        "T√¥i mu·ªën ƒëƒÉng k√Ω ngh·ªâ d√†i h·∫°n cho h·ªçc sinh"
    ]

    model.eval()
    for test_input in test_inputs:
        print(f"\nInput: {test_input}")
        processed_input = preprocess_input(test_input)
        inputs = tokenizer(
            f"chat: {processed_input}",
            return_tensors="pt",
            max_length=MAX_LENGTH,
            padding=True,
            truncation=True
        ).to(device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=MAX_LENGTH,
                num_beams=3,
                early_stopping=True
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"Output: {response}")
        logging.info(f"Test Input: {test_input} -> Output: {response}")

    print("\n‚ú® Training ho√†n t·∫•t!")
    logging.info("Training ho√†n t·∫•t.")

# 7. ENTRY POINT
if __name__ == "__main__":
    main()
